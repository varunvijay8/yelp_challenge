{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cluster_encoded_sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPg80Qxsk4zlNr11jzN9fVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunvijay8/yelp_challenge/blob/master/cluster_encoded_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6gA8I9bOIX3"
      },
      "source": [
        "# Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-R-QHBNrrh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPduYW-rOLsI"
      },
      "source": [
        "\n",
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4YSMdhkszuV"
      },
      "source": [
        "This installs Apache Spark 3.0.0, Java 8, and Findspark, a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bM7vuY4N2-R"
      },
      "source": [
        "#@title Use java 8 since spark does not work well with java 11\n",
        "#!pip install pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgwMT8kSsuV2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4dkp1OsOM8g"
      },
      "source": [
        "#!pip install tf-nightly\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bihroY2dg-Ia"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onqAnoVRueqN"
      },
      "source": [
        "!pip install scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln3bjtwJkm7M"
      },
      "source": [
        "!pip install spacy==2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuBSQ1pZksLA"
      },
      "source": [
        "!spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tE5SOZNtAIb"
      },
      "source": [
        "#Set Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8xegOVitEW4"
      },
      "source": [
        "#@title Set the locations where Spark and Java are installed.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J80nKNxoOUq_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB5er5fe_AgD"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpJpnymmOWaE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZpx3RcFOa8X"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, ArrayType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbiJxWESOdMi"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRs_2tI8Xegk"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk import pos_tag, pos_tag_sents\n",
        "from nltk import RegexpParser\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeZKd4euFQQI"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqedc2xpd564"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSZokzdolbfn"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA7smkmYOicl"
      },
      "source": [
        "YELP_REVIEW_PATH = \"drive/My\\ Drive/yelp_dataset_1/review.json\"\n",
        "YELP_BUSINESS_PATH = \"drive/My\\ Drive/yelp_dataset_1/business.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4rJhmzgPMO2"
      },
      "source": [
        "LABELED_REVIEW_PATH = r\"drive/My Drive/review_1000_sent_token.xlsx\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APl9AC87POKj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1th5KxaPPNl"
      },
      "source": [
        "# Start Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gtmhTYPRYs"
      },
      "source": [
        "spark = pyspark.sql.SparkSession.builder.appName('yelp_eda').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EmiUQmDPTBW"
      },
      "source": [
        "review_df = spark.read.format('json').option(\"inferSchema\", True).load(YELP_REVIEW_PATH)\n",
        "review_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVEOF3eEPVLv"
      },
      "source": [
        "business_df = spark.read.format('json').option(\"inferSchema\", True).load(YELP_BUSINESS_PATH)\n",
        "business_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRix5NzhPeO6"
      },
      "source": [
        "#@title Merge review and restaurant business dataframes on business id\n",
        "restaurant_business = business_df.where('categories like \"%Restaurant%\"').drop('stars')\n",
        "merged_df = review_df.join(restaurant_business, on=\"business_id\", how=\"inner\")\n",
        "merged_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfCyNYdiPnSt"
      },
      "source": [
        "#@title SQL command helper\n",
        "def run_sql(statement):\n",
        "    try:\n",
        "        result = spark.sql(statement)\n",
        "    except Exception as e:\n",
        "        print(e.desc, '\\n', e.stackTrace)\n",
        "        return\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ6HSBKL8cgF"
      },
      "source": [
        "!rm -r spark-warehouse/yelp_dataset.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX5R_G-nPwzv"
      },
      "source": [
        "run_sql('drop database if exists yelp_dataset cascade')\n",
        "run_sql('create database yelp_dataset')\n",
        "dbs = run_sql('show databases')\n",
        "dbs.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5cL8vYPP1jM"
      },
      "source": [
        "#@title Create database\n",
        "permanent_table_name_reviews = \"yelp_dataset.Reviews\"\n",
        "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
        "merged_df.write.mode('overwrite').format(\"parquet\").saveAsTable(permanent_table_name_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw1Ao82SSU5x"
      },
      "source": [
        "run_sql('use yelp_dataset')\n",
        "run_sql('REFRESH table Reviews')\n",
        "tbls = run_sql('show tables')\n",
        "tbls.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88suKs6FQKXx"
      },
      "source": [
        "# Pick random sample of 5000 reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjPSN5YqQF5v"
      },
      "source": [
        "#@title Create dataframe from 5000 randomly picked reviews\n",
        "result = run_sql('''\n",
        "                    SELECT text \n",
        "                    FROM reviews\n",
        "                    ORDER BY RAND()\n",
        "                    LIMIT 5000\n",
        "                 ''')\n",
        "documents_df = result.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylGhoU_kYNWm"
      },
      "source": [
        "# Pre-process review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzy9hEfSoEzd"
      },
      "source": [
        "#@title create a UDF out of sent_tokenize to apply to spark dataframe\n",
        "udf_sent_tokenize = udf(sent_tokenize, ArrayType(StringType())).asNondeterministic()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-3nAtbwnKCS"
      },
      "source": [
        "#@title tokenize reviews to list of sentences, output is a list of rows with reviews tokenized to sentences\n",
        "sentences = result.withColumn('text', udf_sent_tokenize(result.text)).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdVFhWHnU3Ew"
      },
      "source": [
        "#@title flatten tokenzied sentences to list\n",
        "sentences_tokenized = [sent for row in sentences for sent in row.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUElwuYEadEZ"
      },
      "source": [
        "#@title create dataframe from list of sentences\n",
        "sentence_df = pd.DataFrame(sentences_tokenized, columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-EfyL8bvUj"
      },
      "source": [
        "#@title Tokenize reviews to sentences\n",
        "# sentence_df = documents_df.text.apply(sent_tokenize)\n",
        "# tokenized_docs_lst = [l for i, items in sentence_df.iteritems() for l in items]\n",
        "# sentence_df = pd.DataFrame(tokenized_docs_lst, columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0RxtqNKIDvg"
      },
      "source": [
        "#@title Filter out sentences that has less than 3 words\n",
        "sentence_series = sentence_df[sentence_df.text.str.split().apply(len) > 3]\n",
        "sentence_df = pd.DataFrame(sentence_series, columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGWolvBSlHrj"
      },
      "source": [
        "#@title Create instance of spacy\n",
        "spacy_nlp = spacy.load('en', disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bT4484Yk3kZ"
      },
      "source": [
        "#@title Lemmatize the review sentences\n",
        "def lemmatize_sentence(sentence):\n",
        "  sentence_lemma = spacy_nlp(sentence)\n",
        "  return \" \".join([token.text.lower() if token.lemma_ == '-PRON-' else token.lemma_ for token in sentence_lemma])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdMziHmQv5Pt"
      },
      "source": [
        "#@title udf for lemmatize function (not yet used)\n",
        "udf_lemmatize_sentence = udf(lemmatize_sentence, StringType()).asNondeterministic()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD15T64rmw1R"
      },
      "source": [
        "#@title lemmatize sentences\n",
        "lemmatized_sent_series = sentence_df.text.apply(lemmatize_sentence)\n",
        "lemmatized_sent_df = pd.DataFrame(lemmatized_sent_series.to_list(), columns=['review_lemma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDUwsyyz7Xo_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4nf5N-lS7O8"
      },
      "source": [
        "# Create sentence embeddings dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJn6o03oQi8i"
      },
      "source": [
        "#@title Get universal sentence encoder model from TF hub\n",
        "def get_uni_sentence_encoder():\n",
        "  module_url='https://tfhub.dev/google/universal-sentence-encoder-large/5' #@param ['https://tfhub.dev/google/universal-sentence-encoder/3', 'https://tfhub.dev/google/universal-sentence-encoder/4', 'https://tfhub.dev/google/universal-sentence-encoder/5', 'https://tfhub.dev/google/universal-sentence-encoder-large/5']\n",
        "  encoder = hub.load(module_url)\n",
        "  return encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEcthvUpmFTH"
      },
      "source": [
        "uni_sentence_encoder = get_uni_sentence_encoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ex6f1ASaO5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUBOsYT4m1oG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXMy2ei4mmQS"
      },
      "source": [
        "Cluster on lemmatized sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjqhh5FvD1Ec"
      },
      "source": [
        "#@title create grouped dataframes to encode in batches\n",
        "grouped_df = lemmatized_sent_df.groupby(np.arange(len(lemmatized_sent_df)) // 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt8yt3awD1MQ"
      },
      "source": [
        "#@title encode lemmatized sentences in batches\n",
        "encoded_sent_np = np.empty((0,512), dtype=float)\n",
        "for n, gp in grouped_df:\n",
        "  encoded_sent_np = np.append(encoded_sent_np, gp.apply(uni_sentence_encoder).iloc[0], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Zr_EoUD1AZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWTHT-Clt_k0"
      },
      "source": [
        "#@title Create model of AgglomerativeClustering algorithm with euclidean affinity\n",
        "model_for_lemmatized_sent = AgglomerativeClustering(n_clusters=2, affinity='euclidean',linkage='ward')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIPrr64PvRIb"
      },
      "source": [
        "#@title Fit model\n",
        "clusters_lemma = model_for_lemmatized_sent.fit(X=encoded_sent_np)#model_for_lemmatized_sent.fit(X=encoded_lemma_sent_df.iloc[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyEHnRqVvV3d"
      },
      "source": [
        "cluster_lemma_label_df = pd.DataFrame(clusters_lemma.labels_.tolist(), columns=['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pCI86BUvoYy"
      },
      "source": [
        "#@title Label 0 clusters service/experience related sentences\n",
        "non_food_df = sentence_df[(cluster_lemma_label_df.label==0).values]\n",
        "lemmatized_non_food_df = lemmatized_sent_df[(cluster_lemma_label_df.label==0).values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc_yAAvYvvgj"
      },
      "source": [
        "#@title Label 1 clusters food related sentences\n",
        "food_df = sentence_df[(cluster_lemma_label_df.label==1).values]\n",
        "lemmatized_food_df = lemmatized_sent_df[(cluster_lemma_label_df.label==1).values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZYnjNKQ0ctC"
      },
      "source": [
        "lemmatized_non_food_df.tail().review_lemma.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w5mk4pu0cwi"
      },
      "source": [
        "lemmatized_food_df.tail().review_lemma.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4E99y4Rqhrb"
      },
      "source": [
        "#@title encode non-food dataframe\n",
        "grouped_df = lemmatized_non_food_df.groupby(np.arange(len(lemmatized_non_food_df)) // 400)\n",
        "encoded_lemma_non_food_np = np.empty((0,512), dtype=float)\n",
        "for n, gp in grouped_df:\n",
        "  encoded_lemma_non_food_np = np.append(encoded_lemma_non_food_np, gp.apply(uni_sentence_encoder).iloc[0], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc_ycjktqub5"
      },
      "source": [
        "#@title encode food dataframe\n",
        "grouped_df = lemmatized_food_df.groupby(np.arange(len(lemmatized_food_df)) // 400)\n",
        "encoded_lemma_food_np = np.empty((0,512), dtype=float)\n",
        "for n, gp in grouped_df:\n",
        "  encoded_lemma_food_np = np.append(encoded_lemma_food_np, gp.apply(uni_sentence_encoder).iloc[0], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfjcBpebTH5E"
      },
      "source": [
        "lemmatized_food_df.head().review_lemma.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y4Z4RrsTH0z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJH_GvG92hIA"
      },
      "source": [
        "sent = 'we have sesame chicken , general tso , angel blosom , egg roll and hot and sour soup '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rPifM5rTHx7"
      },
      "source": [
        "nlp = spacy.load('en')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbzmJBTNNkMG"
      },
      "source": [
        "doc = nlp(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFiFN28uFg5d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3SgABaM-mA6"
      },
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "  print(chunk.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1eOFJDhNkRZ"
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYc8knMp2cxj"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfwhSjye3z50"
      },
      "source": [
        "df = pd.DataFrame(doc, columns=['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgLnC-50Os3u"
      },
      "source": [
        "def return_pos(token):\n",
        "  return token.pos_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIdnViCcOLha"
      },
      "source": [
        "df[(df.tokens.apply(return_pos) == 'NOUN').to_numpy()].tokens.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0nZtp3Z4tDV"
      },
      "source": [
        "def extract_nouns(sentence, spacy_nlp_obj=nlp):\n",
        "  doc_df = pd.DataFrame(spacy_nlp(sentence), columns=['tokens'])\n",
        "  return doc_df[(doc_df.tokens.apply(return_pos) == 'NOUN').to_numpy()].tokens.to_list()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_bBmSkf4tAX"
      },
      "source": [
        "#@title create series of all nouns from food_df\n",
        "noun_series = food_df.text.apply(extract_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoY5-k16CBeX"
      },
      "source": [
        "#@title extract unique nouns\n",
        "noun_set = set()\n",
        "[noun_set.add(i.string) for items in noun_series for i in items]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dd2_oOH65h_"
      },
      "source": [
        "#@title create dataframe of nouns\n",
        "noun_df = pd.DataFrame(list(noun_set), columns=['nouns'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh8AdxrkDN2t"
      },
      "source": [
        "tok = noun_df.nouns.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDEFhtSKNa97"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRrKYPGFNbD_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNC28rCXNbHm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csSazk7MNlBx"
      },
      "source": [
        "# Filter food entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tikEMCmNkPw"
      },
      "source": [
        "#@title encode all the nouns\n",
        "encoded_noun = noun_df.apply(uni_sentence_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRJ-E8roMSul"
      },
      "source": [
        "#@title create cluster model to filter food nouns\n",
        "model_for_food_noun = AgglomerativeClustering(n_clusters=2, affinity='euclidean',linkage='ward')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEpD4SQnMS3E"
      },
      "source": [
        "clusters_noun = model_for_food_noun.fit(X=encoded_noun.iloc[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1w1jgnjQ2Lx"
      },
      "source": [
        "cluster_noun_df = pd.DataFrame(clusters_noun.labels_.tolist(), columns=['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4wuvIVqMS00"
      },
      "source": [
        "noun_df[(cluster_noun_df.label==0).values].nouns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRT1qYO3MSy7"
      },
      "source": [
        "noun_df[(cluster_noun_df.label==1).values].nouns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgI8pacwU7wH"
      },
      "source": [
        "sent = 'we have biryani, sesame chicken , general tso , angel blosom , egg roll and hot and sour soup '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmpJaKd7ohaK"
      },
      "source": [
        "##@title dictionary of pos tags from spacy to nltk\n",
        "# spacy_nltk_pos = {'CCONJ':'CC', # coordinating conjunction\n",
        "#                   'NUM':'CD', # cardinal digit\n",
        "#                   'DET':'DT', # determiner\n",
        "#                   'PRON':'EX', # existential\n",
        "#                   'X':'FW', # foreign word\n",
        "#                   'ADP':'IN', # preposition/subordinating conjunction\n",
        "#                   'ADJ':'JJ', # adjective \n",
        "#                   'ADJ':'JJR', # adjective, comparative\n",
        "#                   :'JJS', # adjective, superlative\n",
        "#                   :'LS', # list marker\n",
        "#                   :'MD', # modal\n",
        "#                   :'NN', # noun, singular\n",
        "#                   :'NNS', # noun plural\n",
        "#                   :'NNP', # proper noun, singular\n",
        "#                   :'NNPS', # proper noun, plural \n",
        "#                   :'PDT', # predeterminer\n",
        "#                   :'POS', # possessive ending\n",
        "#                   :'POR', # personal pronoun\n",
        "#                   :'PRP$', # possessive pronoun\n",
        "#                   :'RB', # adverb \n",
        "#                   :'RBR', # adverb, comparative\n",
        "#                   :'RBS', # adverb, superlative\n",
        "#                   :'RP', # particle \n",
        "#                   :'TO', # to go ‘to’ the store\n",
        "#                   :'UH', # interjection\n",
        "#                   :'VB', # verb\n",
        "#                   :'VBD', # verb, past tense\n",
        "#                   :'VBG', # verb, gerund/present participle\n",
        "#                   :'VBN', # verb, past participle\n",
        "#                   :'VBP', # verb, sing. present, known-3d take\n",
        "#                   :'VBZ', # verb, 3rd person sing. present takes\n",
        "#                   :'WDT', # wh-determiner which\n",
        "#                   :'WP', # wh-pronoun who, what\n",
        "#                   :'WP$', # possessive wh-pronoun whose\n",
        "#                   :'WRB', # wh-adverb where, when\n",
        "#                   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpxhVNiikUzi"
      },
      "source": [
        "type(pos_tag(word_tokenize(sent)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "778f0zQvlvbr"
      },
      "source": [
        "#@title helper to tag in nltk format but using spacy internally\n",
        "def spacy_pos_tag_to_nltk(sent, spacy_obj):\n",
        "  doc = spacy_obj(sent)\n",
        "  return [(t.text, t.tag_) for t in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB_pz7ZfU73-"
      },
      "source": [
        "#@title helper to chunk nouns\n",
        "def chunk_nouns(sent, spacy_obj):\n",
        "  # tokens = word_tokenize(sent)\n",
        "  # token_tag = pos_tag(tokens)\n",
        "  token_tag = spacy_pos_tag_to_nltk(sent, spacy_obj)\n",
        "  #print(token_tag)\n",
        "  grammar = \"NP: {<JJ>?<CC>?<JJ>?<DT>?<NN.*>+}\"#\"NP: {<JJ>?<DT>?<NN.*>+}\"\n",
        "  noun_parser = RegexpParser(grammar)\n",
        "  return noun_parser.parse(token_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVY3f077U79q"
      },
      "source": [
        "np_chunks = chunk_nouns(sent, nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSlqqB3c9BYS"
      },
      "source": [
        "np_chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOMt7FH39Bsp"
      },
      "source": [
        "#@title helper to chunk nouns 2\n",
        "def chunk_nouns_2(sent, spacy_obj):\n",
        "  lst = []\n",
        "  token_tag = spacy_pos_tag_to_nltk(sent, spacy_obj)\n",
        "  grammar = \"NP: {<NN.*>+}\"#\"NP: {<JJ>?<CC>?<JJ>?<DT>?<NN.*>+}\"\n",
        "  noun_parser = RegexpParser(grammar)\n",
        "  parsed_output = noun_parser.parse(token_tag)\n",
        "  for subtree in parsed_output.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "    lst.append(' '.join(word for (word, pos) in subtree.leaves()))\n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujcR3ZkGLqnV"
      },
      "source": [
        "#@title helper to chunk nouns 3\n",
        "def chunk_nouns_3(sent, spacy_obj):\n",
        "  lst = []\n",
        "  token_tag = spacy_pos_tag_to_nltk(sent, spacy_obj)\n",
        "  #grammar = \"NP: {<JJ>?<CC>?<JJ>?<DT>?<NN.*>+}\"\n",
        "  grammar = r\"\"\"NP: {(<JJ><CC>)?<JJ>?<NN.*>+}\"\"\" #r\"\"\"NP: {(<JJ><CC>)?<JJ>?<DT>?<NN.*>+}\"\"\"\n",
        "  noun_parser = RegexpParser(grammar)\n",
        "  parsed_output = noun_parser.parse(token_tag)\n",
        "  for subtree in parsed_output.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "    lst.append(' '.join(word for (word, pos) in subtree.leaves()))\n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43KEg2TNxahm"
      },
      "source": [
        "#@title filter food phrases with chunk_nouns_3\n",
        "phrases_series = food_df.text.apply(chunk_nouns_3, args=(nlp,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBFYSvPL89Df"
      },
      "source": [
        "#@title extract unique nouns\n",
        "phrase_set = set()\n",
        "[phrase_set.add(i) for items in phrases_series for i in items]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U54pv3sR9Mje"
      },
      "source": [
        "#@title create dataframe of nouns\n",
        "phrase_df = pd.DataFrame(list(phrase_set), columns=['phrases'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjzJGNb5xjre"
      },
      "source": [
        "#@title encode noun phrases\n",
        "encoded_phrases = phrase_df.apply(uni_sentence_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XC9ldwfxjvI"
      },
      "source": [
        "#@title create cluster model to filter food phrases\n",
        "model_for_food_phrases = AgglomerativeClustering(n_clusters=2, affinity='euclidean',linkage='ward')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvVcmaWPRyUE"
      },
      "source": [
        "clusters_phrase = model_for_food_phrases.fit(X=encoded_phrases.iloc[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skud7sL8RyRU"
      },
      "source": [
        "cluster_phrase_df = pd.DataFrame(clusters_phrase.labels_.tolist(), columns=['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlisxezNxkK9"
      },
      "source": [
        "phrase_df[(cluster_phrase_df.label==0).values].phrases.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTS7ym9SB5-N"
      },
      "source": [
        "phrase_df[(cluster_phrase_df.label==1).values].phrases.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4iEqNtc5M8c"
      },
      "source": [
        "len(food_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MbWBPGy5NI5"
      },
      "source": [
        "#@title get aggregated encoding of food phrases\n",
        "encoded_pharses_np = encoded_phrases.iloc[0].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTwZsh2k5NNE"
      },
      "source": [
        "food_pharses_np = encoded_pharses_np[(cluster_phrase_df.label==0).values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beQdn11NFYW0"
      },
      "source": [
        "food_phrases_mean_np = np.mean(food_pharses_np, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfenKVruFYUT"
      },
      "source": [
        "#@title get encoding of food\n",
        "food_encoding = uni_sentence_encoder(['food'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4PrEAk5FYR4"
      },
      "source": [
        "food_np = food_encoding.numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1XIlX76keS7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMJSpw9okeRC"
      },
      "source": [
        "#@title euclidean distance between food and food phrase aggregation\n",
        "np.linalg.norm(food_np - food_phrases_mean_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0qTMTm1kePL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktR43wSXkeM3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIb_crsbkeLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNUp3_ClkeIY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McNPSF3gkeFh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXDPgOJFJBPc"
      },
      "source": [
        "# Filter reviews based on attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9XntnLzWA1x"
      },
      "source": [
        "def euclidean_distance_from_filter(sentence_tensors, filter_tensor):\n",
        "  \"\"\" Returns the euclidean distance betweeen sentence tensors and filter tensor \"\"\"\n",
        "  return tf.map_fn(tf.norm, tf.subtract(sentence_tensors, filter_tensor))\n",
        "\n",
        "def similar_sentences(sent_filter_euclidean_tensor, sent=sentence_df, euclidean_dist=1.5):\n",
        "  \"\"\" Returns list of sentences with euclidean distance less than euclidean_dist from filter \"\"\"\n",
        "  return sent[sent_filter_euclidean_tensor.numpy() < euclidean_dist].text.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKRAvCaaLVAK"
      },
      "source": [
        "def sentence_to_filter_cosine_similarity(sentence, filter: list, correlation=0.75):\n",
        "  \"\"\"Returns True if sentence contains word similar to filter by measuring correlation between words and filter\"\"\"\n",
        "  correlation_tensor = tf.tensordot(uni_sentence_encoder(word_tokenize(sentence)), uni_sentence_encoder(filter), axes= [[1], [1]])\n",
        "  #print(correlation_tensor.numpy())\n",
        "  return any(correlation_tensor.numpy() > correlation)\n",
        "\n",
        "def bigram_sentence_to_filter_cosine_similarity(sentence, filter: list, correlation=0.75):\n",
        "  \"\"\"Returns True if sentence contains bigrams similar to filter by measuring correlation between words and filter\"\"\"\n",
        "  bigram = list(nltk.bigrams(sentence.split()))\n",
        "  bigram = list(map(' '.join, bigram))\n",
        "  correlation_tensor = tf.tensordot(uni_sentence_encoder(bigram), uni_sentence_encoder(filter), axes= [[1], [1]])\n",
        "  #print(correlation_tensor.numpy())\n",
        "  return any(correlation_tensor.numpy() > correlation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rg_-DeJ_IbR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKeS-UCMsp_1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVQwcFibehYu"
      },
      "source": [
        "#@title filter seafood\n",
        "seafood_df = pd.DataFrame(similar_sentences(euclidean_distance_from_filter(encoded_sent_np, uni_sentence_encoder([\"seafood\"])), euclidean_dist=1.25), columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYZjY0nMNs0M"
      },
      "source": [
        "#seafood_mask_series = lemmatized_sent_df.review_lemma.apply(sentence_to_filter_cosine_similarity, args=(['seafood'],0.7))\n",
        "seafood_mask_series = seafood_df.text.apply(sentence_to_filter_cosine_similarity, args=(['seafood'],0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zltaq9xXrAbr"
      },
      "source": [
        "seafood_df[seafood_mask_series.to_numpy()].text.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVZ6gIz9rAlS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v50o1lqerAjU"
      },
      "source": [
        "#@title filter \"happy hour\"\n",
        "happy_hour_df = pd.DataFrame(similar_sentences(euclidean_distance_from_filter(encoded_sent_np, uni_sentence_encoder([\"happy hour\"])), euclidean_dist=1.1), columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wo-tqo-rAZS"
      },
      "source": [
        "happy_hour_mask_series = happy_hour_df.text.apply(bigram_sentence_to_filter_cosine_similarity, args=(['happy hour'], 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSPmxDQH4qUU"
      },
      "source": [
        "happy_hour_df[happy_hour_mask_series.to_numpy()].text.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Z_b2yVDxm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66egaJW-FDHQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AcIc2yUFDNK"
      },
      "source": [
        "#@title filter ambience\n",
        "ambience_df = pd.DataFrame(similar_sentences(euclidean_distance_from_filter(encoded_sent_np, uni_sentence_encoder([\"ambience\"])), euclidean_dist=1.25), columns=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGnDAzDSFDKl"
      },
      "source": [
        "ambience_mask_series = ambience_df.text.apply(sentence_to_filter_cosine_similarity, args=(['ambience'],0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs1ba3YpFvgs"
      },
      "source": [
        "ambience_df[ambience_mask_series.to_numpy()].text.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBTasMzKGgEa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAIXmAs9QStu"
      },
      "source": [
        "#@title filter food from food_df\n",
        "food_mask_series = lemmatized_food_df.review_lemma.apply(sentence_to_filter_cosine_similarity, args=(['food'],0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WAr1uW-REIc"
      },
      "source": [
        "food_mask_series.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmM9A564vTri"
      },
      "source": [
        "lemmatized_food_df[food_mask_series.to_numpy()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYLkaVku2zwR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj4iRIs7ibIY"
      },
      "source": [
        "phrase_lst = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyN4Omz31475"
      },
      "source": [
        "string = \"the Waffles\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDSNMxE4dHo"
      },
      "source": [
        "string.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM3OIVEk5duq"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0wFhvAX5aul"
      },
      "source": [
        "#@title Create instance of spacy\n",
        "spacy_nlp = spacy.load('en', disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44bbgX015rUU"
      },
      "source": [
        "#@title Lemmatize the review sentences\n",
        "def lemmatize_sentence(sentence):\n",
        "  sentence_lemma = spacy_nlp(sentence)\n",
        "  return \" \".join([token.text.lower() if token.lemma_ == '-PRON-' else token.lemma_ for token in sentence_lemma])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDT2ahNH5tNH"
      },
      "source": [
        "lemmatize_sentence(string.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub2tP_do5xfp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}